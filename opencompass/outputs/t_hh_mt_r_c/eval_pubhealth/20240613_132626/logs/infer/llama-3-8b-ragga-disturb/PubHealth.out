06/13 13:26:30 - OpenCompass - INFO - Task****** [llama-3-8b-ragga-disturb/PubHealth]
06/13 13:26:30 - OpenCompass - WARNING - %%%%%%%%%%%%%%%%%%%%%%%%%%%%jtjjtjtjtjtjtjt:-1
2024-06-13 13:26:31.130031: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-13 13:26:31.183135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-13 13:26:31.985315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.48s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.31s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.05s/it]
!!!!!!!!!!!!!!!!!!!!IGI
['/data/zfr/finalTest/opencompass/opencompass/tasks/openicl_infer.py', 'tmp/422311_0_params.py']
Building prefix dict from the default dictionary ...
Loading model from cache /data/zfr/.cache/jieba.cache
Loading model cost 0.724 seconds.
Prefix dict has been built successfully.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:09,  1.33s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:09,  1.57s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:04<00:07,  1.45s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:05<00:05,  1.43s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:07<00:04,  1.40s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:08<00:02,  1.25s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:08<00:01,  1.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:09<00:00,  1.08it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:09<00:00,  1.18s/it]
/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
!!!!!!!!!!!!!!!!!!!!jt:{'w_q': 0.3, 'w_d': 0.4, 'w_k': 0.3, 'search_k': 10, 'compression_ratio': 0.4, 'top_k': 5, 'Vector_Store': 'milvus', 'with_retrieval_classification': True, 'search_method': 'hyde_with_hybrid', 'rerank_model': 'MonoT5', 'compression_method': 'recomp', 'repack_method': 'compact'},milvus:4
*****************
init_models
*****************
{'w_q': 0.3, 'w_d': 0.4, 'w_k': 0.3, 'search_k': 10, 'compression_ratio': 0.4, 'top_k': 5, 'Vector_Store': 'milvus', 'with_retrieval_classification': True, 'search_method': 'hyde_with_hybrid', 'rerank_model': 'MonoT5', 'compression_method': 'recomp', 'repack_method': 'compact'}
milvus:4
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/13 13:27:06 - OpenCompass - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^Start inferencing [llama-3-8b-ragga-disturb/PubHealth]
[2024-06-13 13:27:06,446] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [INFO] Starting inference process...
2024-06-13 13:27:06 [INFO] icl_gen_inferencer: Starting inference process...
  0%|          | 0/500 [00:00<?, ?it/s][2024-06-13 13:27:06,566] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [WARNING] shifouï¼Ÿ@@@@@@@@@0
2024-06-13 13:27:06 [WARNING] icl_gen_inferencer: shifouï¼Ÿ@@@@@@@@@0
*****************
retrieving
*****************
cq:Determine whether the given background supports or refutes the claim, and respond with 'SUPPORT' or 'REFUTE'. Do not generate any other content.
Claim: A mother revealed to her child in a letter after her death that she had just one eye because she had donated the other to him.
Answer: 
q:A mother revealed to her child in a letter after her death that she had just one eye because she had donated the other to him.
[2024-06-13 13:27:06,573] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [WARNING] ??????????????final prompt:Background:

Determine whether the given background supports or refutes the claim, and respond with 'SUPPORT' or 'REFUTE'. Do not generate any other content.
Claim: A mother revealed to her child in a letter after her death that she had just one eye because she had donated the other to him.
Answer: 
2024-06-13 13:27:06 [WARNING] icl_gen_inferencer: ??????????????final prompt:Background:

Determine whether the given background supports or refutes the claim, and respond with 'SUPPORT' or 'REFUTE'. Do not generate any other content.
Claim: A mother revealed to her child in a letter after her death that she had just one eye because she had donated the other to him.
Answer: 
/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 1/500 [00:02<16:50,  2.02s/it][2024-06-13 13:27:08,480] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [WARNING] shifouï¼Ÿ@@@@@@@@@0
2024-06-13 13:27:08 [WARNING] icl_gen_inferencer: shifouï¼Ÿ@@@@@@@@@0
*****************
retrieving
*****************
cq:Determine whether the given background supports or refutes the claim, and respond with 'SUPPORT' or 'REFUTE'. Do not generate any other content.
Claim: Study says too many Americans still drink too much.
Answer: 
q:Study says too many Americans still drink too much.
[2024-06-13 13:27:08,487] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [WARNING] ??????????????final prompt:Background:

Determine whether the given background supports or refutes the claim, and respond with 'SUPPORT' or 'REFUTE'. Do not generate any other content.
Claim: Study says too many Americans still drink too much.
Answer: 
2024-06-13 13:27:08 [WARNING] icl_gen_inferencer: ??????????????final prompt:Background:

Determine whether the given background supports or refutes the claim, and respond with 'SUPPORT' or 'REFUTE'. Do not generate any other content.
Claim: Study says too many Americans still drink too much.
Answer: 
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 2/500 [00:02<07:31,  1.10it/s][2024-06-13 13:27:08,604] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [WARNING] shifouï¼Ÿ@@@@@@@@@1
2024-06-13 13:27:08 [WARNING] icl_gen_inferencer: shifouï¼Ÿ@@@@@@@@@1
*****************
retrieving
*****************
cq:Determine whether the given background supports or refutes the claim, and respond with 'SUPPORT' or 'REFUTE'. Do not generate any other content.
Claim: Viral image Says 80% of novel coronavirus cases are "mild.
Answer: 
q:Viral image Says 80% of novel coronavirus cases are "mild.
*****************
searching
*****************
Jun 13, 2024 1:27:08 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>
INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false
/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `30` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
['A viral image circulating on social media claims that 80% of novel coronavirus cases are "mild."\nThe image, which has been shared thousands of times on Facebook, Twitter, and other platforms, features a chart that purports to show the number of confirmed cases of COVID-19, the disease caused by the virus, and the number of "mild" cases.\nThe chart shows that 80% of cases are "mild," while 15% are "severe" and 5% are "critical."\nBut the chart is misleading.\nThe data it uses is from a study published in the New England Journal of Medicine on March 4, 2020, which looked at 44,000 confirmed cases of COVID-19 in China.\nThe study found that 81% of cases were "mild," but it also found that 14% were "severe" and 5% were "critical."\nThe chart, however, only shows the percentage of "mild" cases, and it does not include the percentage of "severe" and "critical" cases.\nThis can make it seem like the vast majority of people who get COVID-19 will have a mild case, when in fact the majority of people who get COVID-19 will have a mild case, but a significant number will have a severe or critical', 'Viral image Says 80% of novel coronavirus cases are "mild.']

Batches:   0%|          | 0/1 [00:00<?, ?it/s][ABatches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 27.39it/s]
E20240613 13:27:45.315059 424366 server.cpp:47] [SERVER][BlockLock][milvus] Process exit
[2024-06-13 13:27:49,343] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -9) local_rank: 0 (pid: 422717) of binary: /data/zfr/anaconda3/envs/joint/bin/python
Traceback (most recent call last):
  File "/data/zfr/anaconda3/envs/joint/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data/zfr/finalTest/opencompass/opencompass/tasks/openicl_infer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-13_13:27:49
  host      : test
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 422717)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 422717
============================================================
