06/13 09:25:55 - OpenCompass - INFO - Task****** [llama-3-8b-ragga-disturb/WikiMultihopQA]
06/13 09:25:55 - OpenCompass - WARNING - %%%%%%%%%%%%%%%%%%%%%%%%%%%%jtjjtjtjtjtjtjt:-1
2024-06-13 09:25:55.802139: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-13 09:25:55.856069: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-13 09:25:56.616072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
!!!!!!!!!!!!!!!!!!!!IGI
['/data/zfr/finalTest/opencompass/opencompass/tasks/openicl_infer.py', 'tmp/306515_0_params.py']
Building prefix dict from the default dictionary ...
Loading model from cache /data/zfr/.cache/jieba.cache
Loading model cost 0.715 seconds.
Prefix dict has been built successfully.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:04,  1.28it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:02<00:03,  1.30it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:03<00:03,  1.27it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:03<00:02,  1.27it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:04<00:01,  1.25it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:05<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.33it/s]
/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
!!!!!!!!!!!!!!!!!!!!jt:{'w_q': 0.3, 'w_d': 0.4, 'w_k': 0.3, 'search_k': 10, 'compression_ratio': 0.4, 'top_k': 5, 'Vector_Store': 'milvus', 'with_retrieval_classification': True, 'search_method': 'hyde_with_hybrid', 'rerank_model': 'RankLLaMA', 'compression_method': 'recomp', 'repack_method': 'sides'},milvus:5
*****************
init_models
*****************
{'w_q': 0.3, 'w_d': 0.4, 'w_k': 0.3, 'search_k': 10, 'compression_ratio': 0.4, 'top_k': 5, 'Vector_Store': 'milvus', 'with_retrieval_classification': True, 'search_method': 'hyde_with_hybrid', 'rerank_model': 'RankLLaMA', 'compression_method': 'recomp', 'repack_method': 'sides'}
milvus:5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
urllib3.exceptions.SSLError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/urllib3/connectionpool.py", line 844, in urlopen
    retries = retries.increment(
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /fangyuan/nq_extractive_compressor/resolve/main/model.safetensors (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/zfr/finalTest/opencompass/opencompass/tasks/openicl_infer.py", line 172, in <module>
    inferencer.run()
  File "/data/zfr/finalTest/opencompass/opencompass/tasks/openicl_infer.py", line 88, in run
    self.dataset = build_dataset_from_cfg(self.dataset_cfg)
  File "/data/zfr/finalTest/opencompass/opencompass/utils/build.py", line 13, in build_dataset_from_cfg
    return LOAD_DATASET.build(dataset_cfg)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/mmengine/registry/registry.py", line 570, in build
    return self.build_func(cfg, *args, **kwargs, registry=self)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/mmengine/registry/build_functions.py", line 98, in build_from_cfg
    obj_cls = registry.get(obj_type)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/mmengine/registry/registry.py", line 451, in get
    self.import_from_location()
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/mmengine/registry/registry.py", line 376, in import_from_location
    import_module(loc)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/data/zfr/finalTest/opencompass/opencompass/datasets/__init__.py", line 1, in <module>
    from .advglue import *  # noqa: F401, F403
  File "/data/zfr/finalTest/opencompass/opencompass/datasets/advglue.py", line 6, in <module>
    from opencompass.openicl.icl_evaluator import AccEvaluator
  File "/data/zfr/finalTest/opencompass/opencompass/openicl/__init__.py", line 2, in <module>
    from .icl_evaluator import *  # noqa
  File "/data/zfr/finalTest/opencompass/opencompass/openicl/icl_evaluator/__init__.py", line 13, in <module>
    from .lm_evaluator import LMEvaluator  # noqa
  File "/data/zfr/finalTest/opencompass/opencompass/openicl/icl_evaluator/lm_evaluator.py", line 10, in <module>
    from opencompass.openicl.icl_inferencer import GenInferencer
  File "/data/zfr/finalTest/opencompass/opencompass/openicl/icl_inferencer/__init__.py", line 6, in <module>
    from .icl_gen_inferencer import GenInferencer  # noqa
  File "/data/zfr/finalTest/opencompass/opencompass/openicl/icl_inferencer/icl_gen_inferencer.py", line 54, in <module>
    test_retrieval = jt.JointRetrieval(device="cuda", retrieval_config=retrieval_config, milvus_id=milvus_id)
  File "/data/zfr/finalTest/opencompass/JointTest/JointRetrival2.py", line 105, in __init__
    ) = self.init_models()
  File "/data/zfr/finalTest/opencompass/JointTest/JointRetrival2.py", line 167, in init_models
    compressor_model, compressor_tokenizer = init_Compressor_model()
  File "/data/zfr/finalTest/opencompass/JointTest/JointRetrival2.py", line 120, in init_Compressor_model
    model = AutoModel.from_pretrained("fangyuan/nq_extractive_compressor").to("cuda:0")
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3415, in from_pretrained
    if not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs):
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/transformers/utils/hub.py", line 629, in has_file
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=10)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/requests/api.py", line 100, in head
    return request("head", url, **kwargs)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /fangyuan/nq_extractive_compressor/resolve/main/model.safetensors (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))
[2024-06-13 09:26:49,054] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 306732) of binary: /data/zfr/anaconda3/envs/joint/bin/python
Traceback (most recent call last):
  File "/data/zfr/anaconda3/envs/joint/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/zfr/anaconda3/envs/joint/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data/zfr/finalTest/opencompass/opencompass/tasks/openicl_infer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-13_09:26:49
  host      : test
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 306732)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
